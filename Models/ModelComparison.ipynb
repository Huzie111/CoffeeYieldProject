{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c88109b-d7e0-41de-b4eb-087c6ea11c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, TimeSeriesSplit, train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ebb424d-2d0a-4d05-a96f-c919a6d3410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional XGBoost\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except Exception:\n",
    "    XGBOOST_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4abd41ad-0054-42db-8141-2343d100185e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuration\n",
    "\n",
    "CSV_FILE = \"UGANDA_COFFEE_ULTIMATE_DATASET_1961_2024.csv\"  # expected CSV file path (same dir)\n",
    "DATE_COL = \"Year\"\n",
    "TARGET_CONT = \"Yield_kgha\"  # continuous target used to derive classes\n",
    "# Features to include in modeling. These are domain-relevant (climate, production, NDVI, incidence).\n",
    "FEATURES_KEEP = [\n",
    "    \"Production_1000t\",\n",
    "    \"Area_1000ha\",\n",
    "    \"Rainfall_mm\",\n",
    "    \"Temp_C\",\n",
    "    \"NDVI\",\n",
    "    \"Bearing_Trees_Millions\",\n",
    "    \"ONI_lagged9\",\n",
    "    \"Incidence_Index_pct\",\n",
    "]\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9f3d273-9542-4cbc-99d8-29d55fc1c845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directories\n",
    "RESULTS_DIR = Path(\"classification_results\")\n",
    "MODELS_DIR = RESULTS_DIR / \"models\"\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "MODELS_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50978f1d-1c9c-4b28-b82f-c60fa56257ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility functions \n",
    "def create_yield_classes(series: pd.Series, method: str = \"tertile\"):\n",
    "    # Convert continuous yield values into categorical classes using quantile based approach\n",
    "    if method == \"tertile\":\n",
    "        labels = [\"Low\", \"Medium\", \"High\"]\n",
    "        # pd.qcut ensures approximately equal counts per class \n",
    "        return pd.qcut(series, q=3, labels=labels)\n",
    "    elif method == \"custom\":\n",
    "        # Example thresholds — edit to match domain knowledge if available:\n",
    "        bins = [-float(\"inf\"), 500, 1500, float(\"inf\")]\n",
    "        labels = [\"Low\", \"Medium\", \"High\"]\n",
    "        return pd.cut(series, bins=bins, labels=labels)\n",
    "    else:\n",
    "        raise ValueError(\"unknown method for create_yield_classes\")\n",
    "\n",
    "\n",
    "def get_classifiers(random_state=RANDOM_STATE):\n",
    "  \n",
    "    clfs = {\n",
    "        \"LogisticRegression\": Pipeline(\n",
    "            [\n",
    "                # scale numeric features before LR to improve numerical stability and interpretability\n",
    "                (\"scaler\", StandardScaler()),\n",
    "                (\"clf\", LogisticRegression(max_iter=2000, random_state=random_state)),\n",
    "            ]\n",
    "        ),\n",
    "        \"DecisionTree\": Pipeline(\n",
    "            [\n",
    "                # DecisionTree doesn't require scaling so we omit scaler to keep a minimal pipeline\n",
    "                (\"clf\", DecisionTreeClassifier(random_state=random_state)),\n",
    "            ]\n",
    "        ),\n",
    "        \"RandomForest\": Pipeline(\n",
    "            [\n",
    "                (\"clf\", RandomForestClassifier(n_estimators=200, random_state=random_state)),\n",
    "            ]\n",
    "        ),\n",
    "    }\n",
    "    if XGBOOST_AVAILABLE:\n",
    "        clfs[\"XGBoost\"] = Pipeline(\n",
    "            [\n",
    "                (\"clf\", XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\", random_state=random_state)),\n",
    "            ]\n",
    "        )\n",
    "    return clfs\n",
    "\n",
    "\n",
    "def evaluate_stratified_cv(clf, X, y_enc, cv=5):\n",
    "    \n",
    "   # Evaluate a classifier using stratified K-fold CV with both accuracy and F1_macro.We use StratifiedKFold to preserve class balance in each fold.\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=RANDOM_STATE)\n",
    "    # cross_val_score runs fit/predict under the hood and returns per-fold scores\n",
    "    acc_scores = cross_val_score(clf, X, y_enc, cv=skf, scoring=\"accuracy\", n_jobs=-1)\n",
    "    f1_scores = cross_val_score(clf, X, y_enc, cv=skf, scoring=\"f1_macro\", n_jobs=-1)\n",
    "    return {\"cv_accuracy_mean\": float(acc_scores.mean()), \"cv_f1_macro_mean\": float(f1_scores.mean())}\n",
    "\n",
    "\n",
    "def evaluate_holdout(clf, X_train, X_test, y_train_enc, y_test_enc):\n",
    "    \n",
    "    #Train clf on X_train and evaluate on X_test.Returns a small dict of metrics and the predicted labels (ypred).\n",
    "    clf.fit(X_train, y_train_enc)\n",
    "    ypred = clf.predict(X_test)\n",
    "    return {\n",
    "        \"test_accuracy\": float(accuracy_score(y_test_enc, ypred)),\n",
    "        \"test_f1_macro\": float(f1_score(y_test_enc, ypred, average=\"macro\")),\n",
    "    }, ypred\n",
    "\n",
    "\n",
    "def evaluate_time_series(clf, X_df, y_enc, n_splits=4):\n",
    "    \n",
    "    #TimeSeriesSplit evaluation:Splits data by index order,trains on past folds and evaluates on subsequent folds.\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    accs = []\n",
    "    f1s = []\n",
    "    # TimeSeriesSplit yields indices that progress forward — ensure chronology\n",
    "    for train_idx, test_idx in tscv.split(X_df):\n",
    "        Xtr, Xte = X_df.iloc[train_idx], X_df.iloc[test_idx]\n",
    "        ytr, yte = y_enc[train_idx], y_enc[test_idx]\n",
    "        # Fit on only the training chunk (past) and evaluate on the test chunk (future)\n",
    "        clf.fit(Xtr, ytr)\n",
    "        ypred = clf.predict(Xte)\n",
    "        accs.append(accuracy_score(yte, ypred))\n",
    "        f1s.append(f1_score(yte, ypred, average=\"macro\"))\n",
    "    return {\"ts_accuracy_mean\": float(np.mean(accs)), \"ts_f1_macro_mean\": float(np.mean(f1s))}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c945de06-bd6c-4bb9-956d-13d7679bde70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running classifiers: ['LogisticRegression', 'DecisionTree', 'RandomForest', 'XGBoost']\n",
      "\n",
      "--- Evaluating LogisticRegression ---\n",
      "Stratified CV -> accuracy: 0.8897 f1_macro: 0.8807\n",
      "Hold-out test -> accuracy: 0.9231 f1_macro: 0.9259\n",
      "Time-series CV failed: This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int64(1)\n",
      "\n",
      "--- Evaluating DecisionTree ---\n",
      "Stratified CV -> accuracy: 0.8923 f1_macro: 0.8886\n",
      "Hold-out test -> accuracy: 1.0000 f1_macro: 1.0000\n",
      "Time-series CV -> accuracy: 0.6667 f1_macro: 0.4716\n",
      "\n",
      "--- Evaluating RandomForest ---\n",
      "Stratified CV -> accuracy: 0.9231 f1_macro: 0.9194\n",
      "Hold-out test -> accuracy: 1.0000 f1_macro: 1.0000\n",
      "Time-series CV -> accuracy: 0.6875 f1_macro: 0.5154\n",
      "\n",
      "--- Evaluating XGBoost ---\n",
      "Stratified CV -> accuracy: 0.9218 f1_macro: 0.9172\n",
      "Hold-out test -> accuracy: 1.0000 f1_macro: 1.0000\n",
      "Time-series CV failed: Invalid classes inferred from unique values of `y`.  Expected: [0], got [1]\n",
      "\n",
      "\n",
      "Summary (sorted by test_f1_macro if available):\n",
      "             model  cv_accuracy_mean  cv_f1_macro_mean  test_accuracy  test_f1_macro  ts_accuracy_mean  ts_f1_macro_mean\n",
      "      DecisionTree          0.892308          0.888595       1.000000       1.000000          0.666667          0.471591\n",
      "      RandomForest          0.923077          0.919365       1.000000       1.000000          0.687500          0.515399\n",
      "           XGBoost          0.921795          0.917249       1.000000       1.000000         -1.000000         -1.000000\n",
      "LogisticRegression          0.889744          0.880741       0.923077       0.925926         -1.000000         -1.000000\n",
      "\n",
      "Best models by metric:\n",
      "  test_accuracy: DecisionTree -> 1.0000\n",
      "  test_f1_macro: DecisionTree -> 1.0000\n",
      "  cv_accuracy_mean: RandomForest -> 0.9231\n",
      "  cv_f1_macro_mean: RandomForest -> 0.9194\n",
      "  ts_accuracy_mean: RandomForest -> 0.6875\n",
      "  ts_f1_macro_mean: RandomForest -> 0.5154\n",
      "\n",
      "Saved detailed results to classification_results\\yield_classification_results.json\n",
      "Saved trained models (full-data fit) to classification_results\\models (one file per classifier)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    if not Path(CSV_FILE).exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"{CSV_FILE} not found. Place the dataset CSV in the same directory as this script and re-run.\"\n",
    "        )\n",
    "\n",
    "    # Load data and ensure chronological order )\n",
    "    df = pd.read_csv(CSV_FILE)\n",
    "    df = df.sort_values(DATE_COL).reset_index(drop=True)\n",
    "\n",
    "    # Convert non-year columns to numeric (coerce errors -> NaN). This ensures types are correct.\n",
    "    for c in df.columns:\n",
    "        if c != DATE_COL:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # Create categorical yield target\n",
    "    df = df.copy()\n",
    "    df[\"Yield_class\"] = create_yield_classes(df[TARGET_CONT], method=\"tertile\")\n",
    "\n",
    "    # Pick features and drop rows with missing values for simplicity.\n",
    "    \n",
    "    features = [f for f in FEATURES_KEEP if f in df.columns]\n",
    "    model_df = df[[DATE_COL] + features + [\"Yield_class\"]].dropna().reset_index(drop=True)\n",
    "\n",
    "    # Warn if dataset becomes small after dropping missing values\n",
    "    if model_df.shape[0] < 20:\n",
    "        print(\"Warning: small dataset after dropping missing values:\", model_df.shape)\n",
    "\n",
    "    # Feature matrix and target (categorical)\n",
    "    X = model_df[features].copy()\n",
    "    y = model_df[\"Yield_class\"].copy()\n",
    "\n",
    "    # Label-encode target to integers for classifiers that require numeric labels\n",
    "    le = LabelEncoder()\n",
    "    y_enc = le.fit_transform(y)\n",
    "    label_map = {int(i): str(c) for i, c in enumerate(le.classes_)}\n",
    "\n",
    "    # Stratified hold-out split (random). This is a common evaluation but can leak time information.\n",
    "    # If forecasting is the goal, prefer TimeSeriesSplit for final evaluation.\n",
    "    X_train, X_test, y_train_enc, y_test_enc = train_test_split(\n",
    "        X, y_enc, test_size=0.2, random_state=RANDOM_STATE, stratify=y_enc\n",
    "    )\n",
    "\n",
    "    # Prepare chronological data for time-series CV: keep model_df order (already sorted)\n",
    "    X_time = model_df[features].reset_index(drop=True)\n",
    "    y_time_enc = le.transform(model_df[\"Yield_class\"].values)\n",
    "\n",
    "    classifiers = get_classifiers()\n",
    "\n",
    "    # Container for all results\n",
    "    all_results = {}\n",
    "    print(\"Running classifiers:\", list(classifiers.keys()))\n",
    "\n",
    "    for name, clf in classifiers.items():\n",
    "        print(f\"\\n--- Evaluating {name} ---\")\n",
    "        res = {}\n",
    "\n",
    "        # 1) Stratified CV (random folds) - quick check for discoverable patterns\n",
    "        try:\n",
    "            cv_res = evaluate_stratified_cv(clf, X, y_enc, cv=5)\n",
    "            res.update(cv_res)\n",
    "            print(\n",
    "                \"Stratified CV -> accuracy:\",\n",
    "                f\"{cv_res['cv_accuracy_mean']:.4f}\",\n",
    "                \"f1_macro:\",\n",
    "                f\"{cv_res['cv_f1_macro_mean']:.4f}\",\n",
    "            )\n",
    "        except Exception as e:\n",
    "            # Catch exceptions (e.g., if classifier cannot be cross-validated for some reason)\n",
    "            print(\"Stratified CV failed:\", e)\n",
    "\n",
    "        # 2) Hold-out test (train on X_train, evaluate on X_test)\n",
    "        try:\n",
    "            test_res, ypred = evaluate_holdout(clf, X_train, X_test, y_train_enc, y_test_enc)\n",
    "            res.update(test_res)\n",
    "            print(\n",
    "                \"Hold-out test -> accuracy:\",\n",
    "                f\"{test_res['test_accuracy']:.4f}\",\n",
    "                \"f1_macro:\",\n",
    "                f\"{test_res['test_f1_macro']:.4f}\",\n",
    "            )\n",
    "            # Save model trained on the entire feature set (X, y_enc) for convenience (production use)\n",
    "            # Important: This model is trained on all data (including what was used for evaluation).\n",
    "            # If you want to keep a strictly \"only-trained-on-train-set\" model, fit on X_train only.\n",
    "            clf.fit(X, y_enc)\n",
    "            joblib.dump(clf, MODELS_DIR / f\"{name}_full.joblib\")\n",
    "        except Exception as e:\n",
    "            print(\"Hold-out evaluation failed:\", e)\n",
    "\n",
    "        # 3) Time-series CV (no leakage) — recommended when forecasting into the future\n",
    "        try:\n",
    "            ts_res = evaluate_time_series(clf, X_time, y_time_enc, n_splits=4)\n",
    "            res.update(ts_res)\n",
    "            print(\n",
    "                \"Time-series CV -> accuracy:\",\n",
    "                f\"{ts_res['ts_accuracy_mean']:.4f}\",\n",
    "                \"f1_macro:\",\n",
    "                f\"{ts_res['ts_f1_macro_mean']:.4f}\",\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\"Time-series CV failed:\", e)\n",
    "\n",
    "        # Store results for this classifier\n",
    "        all_results[name] = res\n",
    "\n",
    "    # Summarize results into a DataFrame and identify best models per metric\n",
    "    summary = []\n",
    "    for name, metrics in all_results.items():\n",
    "        entry = {\"model\": name}\n",
    "        entry.update(metrics)\n",
    "        summary.append(entry)\n",
    "    summary_df = pd.DataFrame(summary).fillna(-1)\n",
    "\n",
    "    # Sort by test_f1_macro when available (practical metric for balanced multiclass)\n",
    "    if \"test_f1_macro\" in summary_df.columns:\n",
    "        summary_df = summary_df.sort_values(by=\"test_f1_macro\", ascending=False)\n",
    "    else:\n",
    "        summary_df = summary_df.sort_values(by=list(summary_df.columns[1:])[0], ascending=False)\n",
    "\n",
    "    # Identify best model for each candidate metric\n",
    "    best_by = {}\n",
    "    metric_candidates = [\n",
    "        \"test_accuracy\",\n",
    "        \"test_f1_macro\",\n",
    "        \"cv_accuracy_mean\",\n",
    "        \"cv_f1_macro_mean\",\n",
    "        \"ts_accuracy_mean\",\n",
    "        \"ts_f1_macro_mean\",\n",
    "    ]\n",
    "    for metric in metric_candidates:\n",
    "        if metric in summary_df.columns:\n",
    "            # idxmax will raise if column is all -1/NaN, guard with try/except\n",
    "            try:\n",
    "                best_row = summary_df.loc[summary_df[metric].idxmax()]\n",
    "                best_by[metric] = {\"model\": best_row[\"model\"], metric: float(best_row[metric])}\n",
    "            except Exception:\n",
    "                # ignore metrics that could not be computed\n",
    "                pass\n",
    "\n",
    "    # Save metrics & best-by info\n",
    "    out_metrics = {\"label_mapping\": label_map, \"results\": all_results, \"best_by_metric\": best_by}\n",
    "    out_path = RESULTS_DIR / \"yield_classification_results.json\"\n",
    "    with open(out_path, \"w\") as f:\n",
    "        json.dump(out_metrics, f, indent=2)\n",
    "\n",
    "    # Print a human-readable summary\n",
    "    print(\"\\n\\nSummary (sorted by test_f1_macro if available):\")\n",
    "    if summary_df.empty:\n",
    "        print(\"No results to display.\")\n",
    "    else:\n",
    "        print(summary_df.to_string(index=False))\n",
    "\n",
    "    print(\"\\nBest models by metric:\")\n",
    "    if not best_by:\n",
    "        print(\"No best-by metrics computed (columns missing or all metrics failed).\")\n",
    "    else:\n",
    "        for m, v in best_by.items():\n",
    "            print(f\"  {m}: {v['model']} -> {v[m]:.4f}\")\n",
    "\n",
    "    print(f\"\\nSaved detailed results to {out_path}\")\n",
    "    print(f\"Saved trained models (full-data fit) to {MODELS_DIR} (one file per classifier)\")\n",
    "\n",
    "    # Return results dictionary for programmatic consumption if needed\n",
    "    return out_metrics\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f6239a-d5b2-4ded-9f77-9fce64048641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faf4bc0-c5f8-408c-ac5e-c8adff634f58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
